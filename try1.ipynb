{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76e525",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 33 and 15 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, sequential_6_1/dense_76_1/BiasAdd)' with input shapes: [?,33], [?,15].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 108\u001b[0m\n\u001b[0;32m     71\u001b[0m modelq \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     72\u001b[0m     layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m33\u001b[39m, \u001b[38;5;241m15\u001b[39m)),\n\u001b[0;32m     73\u001b[0m     layers\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m15\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    101\u001b[0m ])\n\u001b[0;32m    103\u001b[0m modelq\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[0;32m    104\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    105\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 108\u001b[0m \u001b[43mmodelq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m               \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_test_scaled\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Option \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    112\u001b[0m     Process_parameters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(Process_parameters, ((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, constant_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Shalvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Shalvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\losses\\losses.py:1679\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   1677\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_true, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1678\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[1;32m-> 1679\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmean(ops\u001b[38;5;241m.\u001b[39msquare(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 33 and 15 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, sequential_6_1/dense_76_1/BiasAdd)' with input shapes: [?,33], [?,15]."
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "    \n",
    "#Load Data\n",
    "D = np.loadtxt('./coal_percentages.csv', delimiter=',')\n",
    "P = np.genfromtxt('./Individual_coal_properties.csv', delimiter=',', filling_values=np.nan)\n",
    "Coke_properties = np.loadtxt('./coke_properties.csv', delimiter=',')\n",
    "\n",
    "Option = 1\n",
    "Process_parameters = []\n",
    "if Option == 1:\n",
    "    Process_parameters = np.loadtxt('./Process_parameter_for_Rec_Top_Char.csv', delimiter=',')\n",
    "elif Option == 2:\n",
    "    Process_parameters = np.loadtxt('./Process_parameter_for_Rec_Stam_Char.csv', delimiter=',')\n",
    "elif Option == 3:\n",
    "    Process_parameters = np.loadtxt('./Process_parameter_for_Non_Rec_Stam_Char.csv', delimiter=',')\n",
    "\n",
    "get_coal_count = 33\n",
    "\n",
    "\n",
    "# Compute daily vectors\n",
    "D_tensor = tf.constant(D, dtype=tf.float32)\n",
    "P_tensor = tf.constant(P, dtype=tf.float32)\n",
    "\n",
    "daily_vectors = []\n",
    "for i in range(D_tensor.shape[0]):\n",
    "    row_vector = []\n",
    "    for j in range(P_tensor.shape[1]):\n",
    "        product_vector = tf.multiply(D_tensor[i], P_tensor[:, j])\n",
    "        row_vector.append(product_vector)\n",
    "    daily_vectors.append(tf.stack(row_vector))\n",
    "\n",
    "daily_vectors_tensor = tf.stack(daily_vectors)\n",
    "\n",
    "\n",
    "daily_vectors_flattened = daily_vectors_tensor.numpy().reshape(74, -1)\n",
    "Blended_coal_parameters = np.loadtxt('./blended_coal_data.csv', delimiter=',')\n",
    "\n",
    "input_train, input_test, target_train, target_test = train_test_split(\n",
    "    daily_vectors_tensor.numpy(), Blended_coal_parameters, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scaling\n",
    "input_scaler = MinMaxScaler()\n",
    "output_scaler = MinMaxScaler()\n",
    "\n",
    "input_train_reshaped = input_train.reshape(input_train.shape[0], -1)\n",
    "input_test_reshaped = input_test.reshape(input_test.shape[0], -1)\n",
    "\n",
    "input_train_scaled = input_scaler.fit_transform(input_train_reshaped)\n",
    "input_test_scaled = input_scaler.transform(input_test_reshaped)\n",
    "input_train_scaled = input_train_scaled.reshape(-1, 33, 15)\n",
    "input_test_scaled = input_test_scaled.reshape(-1, 33, 15)\n",
    "\n",
    "\n",
    "target_train_scaled = output_scaler.fit_transform(target_train)\n",
    "target_test_scaled = output_scaler.transform(target_test)\n",
    "\n",
    "input_train_scaled = input_train_scaled.reshape(input_train.shape)\n",
    "input_test_scaled = input_test_scaled.reshape(input_test.shape)\n",
    "input_train_scaled = input_train_scaled.reshape(-1, 33, 15)\n",
    "input_test_scaled = input_test_scaled.reshape(-1, 33, 15)\n",
    "\n",
    "\n",
    "# Define model\n",
    "modelq = keras.Sequential([\n",
    "    layers.Input(shape=(33, 15)),\n",
    "    layers.Flatten(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='leaky_relu', kernel_initializer='he_normal'),\n",
    "    layers.LayerNormalization(),\n",
    "\n",
    "    layers.Dense(256, activation='tanh'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='leaky_relu', kernel_initializer='he_normal'),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='swish', kernel_initializer='he_normal'),\n",
    "    layers.LayerNormalization(),\n",
    "\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    layers.Dense(64, activation='swish', kernel_initializer='he_normal'),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Dense(32, activation='swish', kernel_initializer='he_normal'),\n",
    "    layers.LayerNormalization(),\n",
    "    layers.Dense(15, activation='linear')\n",
    "])\n",
    "\n",
    "modelq.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['mae'])\n",
    "modelq.summary()\n",
    "\n",
    "modelq.fit(input_train_scaled, target_train_scaled, epochs=50, batch_size=8,\n",
    "               validation_data=(input_test_scaled, target_test_scaled), verbose=0)\n",
    "\n",
    "if Option == 3:\n",
    "    Process_parameters = np.pad(Process_parameters, ((0, 0), (0, 2)), mode='constant', constant_values=0)\n",
    "\n",
    "Conv_matrix = Blended_coal_parameters + Process_parameters\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Conv_matrix, Coke_properties, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling second phase\n",
    "\n",
    "input__scaler = MinMaxScaler()\n",
    "output__scaler = MinMaxScaler()\n",
    "input_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "input_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "input_train_scaled = input__scaler.fit_transform(input_train_reshaped)\n",
    "input_test_scaled = input__scaler.transform(input_test_reshaped)\n",
    "\n",
    "target_train_scaled = output__scaler.fit_transform(y_train)\n",
    "target_test_scaled = output__scaler.transform(y_test)\n",
    "\n",
    " # Build and train second model\n",
    "rf_model = keras.Sequential([\n",
    "    layers.Input(shape=(15, 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='leaky_relu', kernel_initializer='he_normal'),\n",
    "    layers.LayerNormalization(),\n",
    "    layers.Dense(256, activation='tanh'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='leaky_relu', kernel_initializer='he_normal'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='swish', kernel_initializer='he_normal'),\n",
    "    layers.LayerNormalization(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='swish', kernel_initializer='he_normal'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(32, activation='swish', kernel_initializer='he_normal'),\n",
    "    layers.LayerNormalization(),\n",
    "    layers.Dense(15, activation='linear')\n",
    "])\n",
    "\n",
    "rf_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                    loss='mse', metrics=['mae'])\n",
    "\n",
    "rf_model.fit(input_train_scaled, target_train_scaled, epochs=100, batch_size=8,\n",
    "                validation_data=(input_test_scaled, target_test_scaled), verbose=0)\n",
    "\n",
    "min_percentages = np.array([35, 25, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "max_percentages = np.array([45, 35, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "#Get all weightages for coke properties\n",
    "ash_weightage = 1\n",
    "vm_weightage = 1 #i think not needed \n",
    "m40_weightage = 1\n",
    "m10_weightage = 1\n",
    "csn_weightage = 1\n",
    "cri_weightage = 1\n",
    "ams_weightage = 1\n",
    "\n",
    "\n",
    "ash_min = 15\n",
    "ash_max = 17\n",
    "vm_min = 0.5\n",
    "vm_max = 1\n",
    "m40_min = 90\n",
    "m40_max = 93\n",
    "m10_min = 5\n",
    "m10_max = 7\n",
    "csn_min = 65\n",
    "csn_max = 70\n",
    "cri_min = 22\n",
    "cri_max = 26\n",
    "ams_min = 53\n",
    "ams_max = 56\n",
    "\n",
    "#desired coke parameters\n",
    "desired_ash = 15.5 #less\n",
    "desired_VM = 0.75   #less\n",
    "desired_m40 = 91.5   #higher\n",
    "desired_m10 = 6   #less\n",
    "desired_csr = 67.5   #higher\n",
    "desired_cri = 24  #less\n",
    "desired_ams = 54.5\n",
    "desired_coke_parameters = [desired_ash,desired_VM,4,0.8,1.54,19,33,24,6,desired_m40,desired_m10,4,desired_csr,desired_cri,desired_ams]\n",
    "\n",
    "\n",
    "def generate_combinations(index, current_combination, current_sum):\n",
    "        target_sum = 100\n",
    "        if index == len(min_percentages) - 1:\n",
    "            remaining = target_sum - current_sum\n",
    "            if min_percentages[index] <= remaining <= max_percentages[index]:\n",
    "                yield current_combination + [remaining]\n",
    "            return\n",
    "        for value in range(min_percentages[index], max_percentages[index] + 1):\n",
    "            if current_sum + value <= target_sum:\n",
    "                yield from generate_combinations(index + 1, current_combination + [value], current_sum + value)\n",
    "\n",
    "all_combinations = np.array(list(generate_combinations(0, [], 0)))\n",
    "\n",
    "proces_para=[17, 10, 0.78, 1198,1255,1214 ,125,100,12.84,75,140,80,20,1050,5]\n",
    "coal_costs = [0] * 33  # Initializing an array with 13 elements\n",
    "\n",
    "# Example input: Replace these values with the actual costs\n",
    "coal_costs = [18757,16700,16900,4500,19000,16209,16209,16209,16209,12000, 5200, 20160, 20160, 20160, 20160, 17000, 17000, 13440, 14200, 13440, 13440, 15000,9240,17000, 17000,17000, 17000,17000, 17000,17000, 17000,17000, 17000]\n",
    "\n",
    "\n",
    "D_ = all_combinations\n",
    "P_ = P\n",
    "      \n",
    "# Prepare tensors for new combinations\n",
    "D_tensor = tf.constant(D_, dtype=tf.float32)\n",
    "P_tensor = tf.constant(P_, dtype=tf.float32)\n",
    "\n",
    "# Compute daily vectors for new combinations\n",
    "daily_vectors = []\n",
    "for i in range(D_tensor.shape[0]):\n",
    "    row_vector = []\n",
    "    for j in range(P_tensor.shape[1]):\n",
    "        product_vector = tf.multiply(D_tensor[i], P_tensor[:, j])\n",
    "        row_vector.append(product_vector)\n",
    "    daily_vectors.append(tf.stack(row_vector))\n",
    "\n",
    "daily_vectors_tensor = tf.stack(daily_vectors)\n",
    "daily_vectors_flattened = daily_vectors_tensor.numpy().reshape(daily_vectors_tensor.shape[0], -1)\n",
    "\n",
    "# Predict blended coal properties using pre-trained model\n",
    "b1_scaled = input_scaler.transform(daily_vectors_flattened)\n",
    "b1_scaled = b1_scaled.reshape(-1, get_coal_count, 15)\n",
    "blend1 = modelq.predict(b1_scaled)\n",
    "blended_coal_properties = output_scaler.inverse_transform(blend1)\n",
    "\n",
    "blend1_with_process = blended_coal_properties + proces_para\n",
    "blend1_scaled = input__scaler.transform(blend1_with_process)\n",
    "coke = rf_model.predict(blend1_scaled)\n",
    "predictions = output__scaler.inverse_transform(coke)\n",
    "\n",
    "def filter_valid_and_invalid(predictions, combinations, blended_coal_properties):\n",
    "    valid_indices = []\n",
    "    invalid_indices = []\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        # Check if all values are within the specified range\n",
    "        if (\n",
    "            ash_min <= prediction[0] <= ash_max and  # ASH\n",
    "            vm_min <= prediction[1] <= vm_max and  # VM\n",
    "            m40_min <= prediction[9] <= m40_max and  # M_40\n",
    "            m10_min <= prediction[10] <= m10_max and  # M_10\n",
    "            csn_min <= prediction[12] <= csn_max and  # CSR\n",
    "            cri_min <= prediction[13] <= cri_max and # CRI\n",
    "            ams_min <= prediction[14] <= ams_max\n",
    "            \n",
    "        ):\n",
    "            valid_indices.append(i)\n",
    "        else:\n",
    "            invalid_indices.append(i)\n",
    "    # Separate valid and invalid predictions, combinations, and blended coal properties\n",
    "    valid_predictions = predictions[valid_indices]\n",
    "    valid_combinations = combinations[valid_indices]\n",
    "    valid_blended_coal_properties = [blended_coal_properties[i] for i in valid_indices]\n",
    "    print(\"valid_blended_coal_properties\",\"\\n\")\n",
    "    print(valid_blended_coal_properties[0],\"\\n\")\n",
    "    print(\"blended_coal_properties\",\"\\n\")\n",
    "    print(blended_coal_properties[0],\"\\n\")\n",
    "    invalid_predictions = predictions[invalid_indices]\n",
    "    invalid_combinations = combinations[invalid_indices]\n",
    "    invalid_blended_coal_properties = [blended_coal_properties[i] for i in invalid_indices]\n",
    "\n",
    "    return (\n",
    "        valid_predictions,\n",
    "        valid_combinations,\n",
    "        valid_blended_coal_properties,\n",
    "        invalid_predictions,\n",
    "        invalid_combinations,\n",
    "        invalid_blended_coal_properties,\n",
    "    )\n",
    "\n",
    "(valid_predictions, valid_combinations, valid_blended_coal_properties,\n",
    "    invalid_predictions, invalid_combinations, invalid_blended_coal_properties) = \\\n",
    "    filter_valid_and_invalid(predictions, all_combinations, blended_coal_properties)\n",
    "\n",
    "predictions = valid_predictions\n",
    "all_combinations = valid_combinations\n",
    "blended_coal_properties = valid_blended_coal_properties\n",
    "\n",
    "# Initialize an array to store differences\n",
    "differences = []\n",
    "prediction = valid_predictions\n",
    "all_combinations = valid_combinations\n",
    "\n",
    "# Calculate differences\n",
    "for prediction in predictions:\n",
    "    diff = []\n",
    "    \n",
    "    # ASH (less is better)\n",
    "    diff.append(((desired_ash - prediction[0]) / desired_ash) * ash_weightage)\n",
    "\n",
    "    # VM (less is better)\n",
    "    diff.append(((desired_VM - prediction[1]) / desired_VM) * vm_weightage)\n",
    "\n",
    "    # M_40 (higher is better)\n",
    "    diff.append(((prediction[9] - desired_m40) / desired_m40) * m40_weightage)\n",
    "\n",
    "    # M_10 (less is better)\n",
    "    diff.append(((desired_m10 - prediction[10]) / desired_m10) * m10_weightage)\n",
    "\n",
    "    # CSR (higher is better)\n",
    "    diff.append(((prediction[12] - desired_csr) / desired_csr) * csn_weightage)\n",
    "\n",
    "    # CRI (less is better)\n",
    "    diff.append(((desired_cri - prediction[13]) / desired_cri) * cri_weightage)\n",
    "    \n",
    "    # AMS (higher is better)\n",
    "    diff.append(((prediction[14] - desired_csr) / desired_csr) * csn_weightage)\n",
    "\n",
    "    differences.append(diff)\n",
    "\n",
    "# Calculate total differences and store in an array\n",
    "total_differences = [sum(diff) for diff in differences]\n",
    "sorted_indices = np.argsort(total_differences)[::-1]\n",
    "\n",
    "sorted_predictions = predictions[sorted_indices]\n",
    "sorted_blends = all_combinations[sorted_indices]\n",
    "sorted_diff = [differences[i] for i in sorted_indices]\n",
    "sorted_blended_coal_properties = [blended_coal_properties[i] for i in sorted_indices]\n",
    "\n",
    "total_costs = [sum(blend[i] * coal_costs[i] / 100 for i in range(32)) for blend in sorted_blends]\n",
    "\n",
    "sorted_indices_by_cost = np.argsort(total_costs)\n",
    "\n",
    "sorted_blend_cost = sorted_blends[sorted_indices_by_cost]\n",
    "sorted_prediction_cost = sorted_predictions[sorted_indices_by_cost]\n",
    "sorted_total_cost = np.array(total_costs)[sorted_indices_by_cost]\n",
    "sorted_blended_coal_properties_cost = [sorted_blended_coal_properties[i] for i in sorted_indices_by_cost]\n",
    "sorted_diff_cost = [sorted_diff[i] for i in sorted_indices_by_cost]\n",
    "\n",
    "# Combine Cost and Performance\n",
    "normalized_costs = (total_costs - np.min(total_costs)) / (np.max(total_costs) - np.min(total_costs))\n",
    "\n",
    "normalized_differences = ((total_differences - np.min(total_differences)) /\n",
    "                            (np.max(total_differences) - np.min(total_differences)))\n",
    "\n",
    "cost_weight = 1\n",
    "performance_weight = 1\n",
    "\n",
    "combined_scores = (cost_weight * normalized_costs) + (performance_weight * normalized_differences)\n",
    "best_combined_index = np.argmin(combined_scores)\n",
    "\n",
    "# Helper for Cost Calculation\n",
    "def calculate_cost(blend, coal_costs):\n",
    "    return sum(blend[i] * coal_costs[i] / 100 for i in range(min(len(blend), len(coal_costs))))\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\n",
    "# 1. Best by performance (first in performance-sorted list)\n",
    "blend_1 = sorted_blends[0]\n",
    "blended_coal_1 = sorted_blended_coal_properties[0]\n",
    "blend_1_properties = sorted_predictions[0]\n",
    "blend_1_cost = calculate_cost(blend_1, coal_costs)\n",
    "\n",
    "print(sorted_blends[50])\n",
    "print(sorted_blends[32])\n",
    "print(sorted_blends[20])\n",
    "\n",
    "print(sorted_predictions[50])\n",
    "print(sorted_predictions[32])\n",
    "print(sorted_predictions[20])\n",
    "\n",
    "\n",
    "\n",
    "# 2. Cheapest (first in cost-sorted list)\n",
    "blend_2 = sorted_blend_cost[0]\n",
    "blended_coal_2 = sorted_blended_coal_properties_cost[0]\n",
    "blend_2_properties = sorted_prediction_cost[0]\n",
    "blend_2_cost = sorted_total_cost[0]\n",
    "\n",
    "# 3. Best combined (from original arrays, using best_combined_index)\n",
    "blend_3 = all_combinations[best_combined_index]\n",
    "blended_coal_3 = valid_blended_coal_properties[best_combined_index]\n",
    "blend_3_properties = valid_predictions[best_combined_index]\n",
    "blend_3_cost = calculate_cost(blend_3, coal_costs)\n",
    "\n",
    "print(\"Blend 1 (First in Sorted Blends):\")\n",
    "print(\"Blend Values in %:\", blend_1)\n",
    "print(\"Properties:\", blend_1_properties,\"\\n\")\n",
    "print(\"Blended Coal Parameters blend 1:\", np.array2string(blended_coal_1, precision=3, suppress_small=True))\n",
    "print(\"Cost:\", blend_1_cost, \"\\n\")\n",
    "\n",
    "print(\"Blend 2 (Lowest Cost):\")\n",
    "print(\"Blend Values in %:\", blend_2)\n",
    "print(\"Properties:\", blend_2_properties)\n",
    "print(\"Blended Coal Parameters blend 2:\", np.array2string(blended_coal_2, precision=3, suppress_small=True))\n",
    "print(\"Cost:\", blend_2_cost, \"\\n\")\n",
    "\n",
    "print(\"Blend 3 (Best Combined):\")\n",
    "print(\"Blend Values:\", blend_3)\n",
    "print(\"Properties:\", blend_3_properties)\n",
    "print(\"Blended Coal Parameters blend 3:\", np.array2string(blended_coal_3, precision=3, suppress_small=True))\n",
    "print(\"Cost:\", blend_3_cost, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f8c2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\n",
      "ERROR: No matching distribution found for itertools\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install itertools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
